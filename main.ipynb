{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MNfKVlm2Py0",
    "outputId": "2a3ffa65-2896-4f30-a643-2588c08033ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.2)\n",
      "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "  Attempting uninstall: typeguard\n",
      "    Found existing installation: typeguard 4.4.1\n",
      "    Uninstalling typeguard-4.4.1:\n",
      "      Successfully uninstalled typeguard-4.4.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n",
      "Collecting PubChemPy\n",
      "  Downloading PubChemPy-1.0.4.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: PubChemPy\n",
      "  Building wheel for PubChemPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for PubChemPy: filename=PubChemPy-1.0.4-py3-none-any.whl size=13819 sha256=7951c12ed378cd1159087267ccc281151668e4b0dd19e172b8eb33b11a356cce\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/7c/45/18a0671e3c3316966ef7ed9ad2b3f3300a7e41d3421a44e799\n",
      "Successfully built PubChemPy\n",
      "Installing collected packages: PubChemPy\n",
      "Successfully installed PubChemPy-1.0.4\n",
      "Collecting openpyxl==3.0.9\n",
      "  Downloading openpyxl-3.0.9-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl==3.0.9) (2.0.0)\n",
      "Downloading openpyxl-3.0.9-py2.py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openpyxl\n",
      "  Attempting uninstall: openpyxl\n",
      "    Found existing installation: openpyxl 3.1.5\n",
      "    Uninstalling openpyxl-3.1.5:\n",
      "      Successfully uninstalled openpyxl-3.1.5\n",
      "Successfully installed openpyxl-3.0.9\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.0.9)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openpyxl\n",
      "  Attempting uninstall: openpyxl\n",
      "    Found existing installation: openpyxl 3.0.9\n",
      "    Uninstalling openpyxl-3.0.9:\n",
      "      Successfully uninstalled openpyxl-3.0.9\n",
      "Successfully installed openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import networkx as nx\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pubchempy import *\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Softmax\n",
    "from keras.layers import Conv2D, GRU\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Masking, RepeatVector, Flatten, \\\n",
    "    Concatenate, Lambda\n",
    "from keras.models import Model\n",
    "from keras.layers import Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import optimizers, layers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def next_step(graph, previous, current, p, q):\n",
    "    neighbors = list(graph.neighbors(current))\n",
    "\n",
    "    weights = []\n",
    "    # Adjust the weights of the edges to the neighbors with respect to p and q.\n",
    "    for neighbor in neighbors:\n",
    "        if neighbor == previous:\n",
    "            # Control the probability to return to the previous node.\n",
    "            weights.append(graph[current][neighbor][\"weight\"] / p)\n",
    "        elif graph.has_edge(neighbor, previous):\n",
    "            # The probability of visiting a local node.\n",
    "            weights.append(graph[current][neighbor][\"weight\"])\n",
    "        else:\n",
    "            # Control the probability to move forward.\n",
    "            weights.append(graph[current][neighbor][\"weight\"] / q)\n",
    "\n",
    "    # Compute the probabilities of visiting each neighbor.\n",
    "    weight_sum = sum(weights)\n",
    "    probabilities = [weight / weight_sum for weight in weights]\n",
    "    # Probabilistically select a neighbor to visit.\n",
    "    next = np.random.choice(neighbors, size=1, p=probabilities)[0]\n",
    "    return next\n",
    "\n",
    "\n",
    "def random_walk(graph, num_walks,vocabulary_lookup, num_steps, p, q):\n",
    "    walks = []\n",
    "    nodes = list(graph.nodes())\n",
    "    # Perform multiple iterations of the random walk.\n",
    "    for walk_iteration in range(num_walks):\n",
    "        random.shuffle(nodes)\n",
    "\n",
    "        for node in tqdm(\n",
    "            nodes,\n",
    "            position=0,\n",
    "            leave=True,\n",
    "            desc=f\"Random walks iteration {walk_iteration + 1} of {num_walks}\",\n",
    "        ):\n",
    "            # Start the walk with a random node from the graph.\n",
    "            walk = [node]\n",
    "            # Randomly walk for num_steps.\n",
    "            while len(walk) < num_steps:\n",
    "                current = walk[-1]\n",
    "                previous = walk[-2] if len(walk) > 1 else None\n",
    "                # Compute the next node to visit.\n",
    "                next = next_step(graph, previous, current, p, q)\n",
    "                walk.append(next)\n",
    "            # Replace node ids (movie ids) in the walk with token ids.\n",
    "            walk = [vocabulary_lookup[token] for token in walk]\n",
    "            # Add the walk to the generated sequence.\n",
    "            walks.append(walk)\n",
    "\n",
    "    return walks\n",
    "\n",
    "\n",
    "def generate_examples(sequences, window_size, num_negative_samples, vocabulary_size):\n",
    "    example_weights = defaultdict(int)\n",
    "    # Iterate over all sequences (walks).\n",
    "    for sequence in tqdm(\n",
    "        sequences,\n",
    "        position=0,\n",
    "        leave=True,\n",
    "        desc=f\"Generating postive and negative examples\",\n",
    "    ):\n",
    "        # Generate positive and negative skip-gram pairs for a sequence (walk).\n",
    "        pairs, labels = keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            window_size=window_size,\n",
    "            negative_samples=num_negative_samples,\n",
    "        )\n",
    "        for idx in range(len(pairs)):\n",
    "            pair = pairs[idx]\n",
    "            label = labels[idx]\n",
    "            target, context = min(pair[0], pair[1]), max(pair[0], pair[1])\n",
    "            if target == context:\n",
    "                continue\n",
    "            entry = (target, context, label)\n",
    "            example_weights[entry] += 1\n",
    "\n",
    "    targets, contexts, labels, weights = [], [], [], []\n",
    "    for entry in example_weights:\n",
    "        weight = example_weights[entry]\n",
    "        target, context, label = entry\n",
    "        targets.append(target)\n",
    "        contexts.append(context)\n",
    "        labels.append(label)\n",
    "        weights.append(weight)\n",
    "\n",
    "    return np.array(targets), np.array(contexts), np.array(labels), np.array(weights)\n",
    "\n",
    "\n",
    "def create_dataset(targets, contexts, labels, weights, batch_size):\n",
    "    inputs = {\n",
    "        \"target\": targets,\n",
    "        \"context\": contexts,\n",
    "    }\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels, weights))\n",
    "    dataset = dataset.shuffle(buffer_size=batch_size * 2)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model(vocabulary_size, embedding_dim):\n",
    "\n",
    "    inputs = {\n",
    "        \"target\": layers.Input(name=\"target\", shape=(), dtype=\"int32\"),\n",
    "        \"context\": layers.Input(name=\"context\", shape=(), dtype=\"int32\"),\n",
    "    }\n",
    "    # Initialize item embeddings.\n",
    "    embed_item = layers.Embedding(\n",
    "        input_dim=vocabulary_size,\n",
    "        output_dim=embedding_dim,\n",
    "        embeddings_initializer=\"he_normal\",\n",
    "        embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        name=\"item_embeddings\",\n",
    "    )\n",
    "    # Lookup embeddings for target.\n",
    "    target_embeddings = embed_item(inputs[\"target\"])\n",
    "    # Lookup embeddings for context.\n",
    "    context_embeddings = embed_item(inputs[\"context\"])\n",
    "    # Compute dot similarity between target and context embeddings.\n",
    "    logits = layers.Dot(axes=1, normalize=False, name=\"dot_similarity\")(\n",
    "        [target_embeddings, context_embeddings]\n",
    "    )\n",
    "    # Create the model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n",
    "\n",
    "def node2vec_on_PPI(protein_protein_graph):\n",
    "    vocabulary_protein = [\"NA\"] + list(protein_protein_graph.nodes)\n",
    "    vocabulary_lookup_protein = {token: idx for idx, token in enumerate(vocabulary_protein)}\n",
    "\n",
    "\n",
    "    ##############################################\n",
    "\n",
    "    # Random walk return parameter.\n",
    "    p = 1\n",
    "    # Random walk in-out parameter.\n",
    "    q = 1\n",
    "    # Number of iterations of random walks.\n",
    "    num_walks = 3\n",
    "    # Number of steps of each random walk.\n",
    "    num_steps = 10\n",
    "\n",
    "    walks_protein = random_walk(protein_protein_graph, num_walks,vocabulary_lookup_protein, num_steps, p, q)\n",
    "\n",
    "\n",
    "    # print(\"Number of protein walks generated:\", len(walks_protein))\n",
    "\n",
    "    #################################################\n",
    "\n",
    "\n",
    "    num_negative_samples = 10#4\n",
    "\n",
    "\n",
    "    # generate examples for Protein-Protein Network\n",
    "\n",
    "    targets_p, contexts_p, labels_p, weights_p = generate_examples(\n",
    "      sequences=walks_protein,\n",
    "      window_size=num_steps,\n",
    "      num_negative_samples=num_negative_samples,\n",
    "      vocabulary_size=len(vocabulary_protein),\n",
    "    )\n",
    "    batch_size = 1024\n",
    "\n",
    "    dataset_protein = create_dataset(\n",
    "      targets=targets_p,\n",
    "      contexts=contexts_p,\n",
    "      labels=labels_p,\n",
    "      weights=weights_p,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    protein_embeding_dim=200\n",
    "\n",
    "\n",
    "    model_protein = create_model(len(vocabulary_protein), protein_embeding_dim)\n",
    "    model_protein.compile(\n",
    "      optimizer=keras.optimizers.Adam(learning_rate),\n",
    "      loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    )\n",
    "\n",
    "    #history_protein = model_protein.fit(dataset_protein, epochs=1)\n",
    "\n",
    "    protein_embeddings = model_protein.get_layer(\"item_embeddings\").get_weights()[0]\n",
    "    # print(\"Protein Embeddings shape:\", protein_embeddings.shape)\n",
    "    return protein_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpVsiFME2Y3g"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import networkx as nx\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#import tensorflow_addons as tfa\n",
    "from pubchempy import *\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Softmax\n",
    "from keras.layers import Conv2D, GRU\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Masking, RepeatVector, Flatten, \\\n",
    "    Concatenate, Lambda\n",
    "from keras.models import Model\n",
    "from keras.layers import Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import optimizers, layers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_list_of_proteins(path_dataset, dataset, idx = 1):\n",
    "    \n",
    "    data = pd.read_excel(path_dataset + dataset + '/protein-protein_network.xlsx')\n",
    "\n",
    "    yy = data.iloc[:, 1]\n",
    "    list_of_prots = yy.unique()\n",
    "\n",
    "    yy = data.iloc[:, 0]\n",
    "    tmp = yy.unique()\n",
    "    list_of_prots = np.concatenate((list_of_prots, tmp))\n",
    "    list_of_prots = np.unique(list_of_prots)\n",
    "\n",
    "    data = pd.read_csv(path_dataset + dataset + '/drug_protein.csv')\n",
    "\n",
    "    yy = data.iloc[:, 0]\n",
    "    list_of_drugs = yy.unique()\n",
    "\n",
    "    yy = data.iloc[:, 1]\n",
    "    tmp = yy.unique()\n",
    "    list_of_prots = np.concatenate((list_of_prots, tmp))\n",
    "    list_of_prots = np.unique(list_of_prots)\n",
    "\n",
    "    data = pd.read_csv(path_dataset + dataset + '/cell_protein.csv')\n",
    "\n",
    "    yy = data.iloc[:, 2]\n",
    "    tmp = yy.unique()\n",
    "    list_of_prots = np.concatenate((list_of_prots, tmp))\n",
    "    list_of_prots = np.unique(list_of_prots)\n",
    "    return list_of_prots\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_list_of_drugs(path_dataset, dataset):\n",
    "    data = pd.read_csv(path_dataset + dataset + '/drug_protein.csv')\n",
    "\n",
    "    yy = data.iloc[:, 0]\n",
    "    list_of_drugs = yy.unique()\n",
    "\n",
    "    data = pd.read_csv(path_dataset + dataset + '/drug_combinations.csv')\n",
    "    if dataset == 'OncologyScreen':\n",
    "      idx = 1\n",
    "    else:\n",
    "      idx = 3\n",
    "    yy = data.iloc[:, idx]\n",
    "    tmp = yy.unique()\n",
    "    list_of_drugs = np.concatenate((list_of_drugs, tmp))\n",
    "    list_of_drugs = np.unique(list_of_drugs)\n",
    "\n",
    "    yy = data.iloc[:, idx+1]\n",
    "    tmp = yy.unique()\n",
    "    list_of_drugs = np.concatenate((list_of_drugs, tmp))\n",
    "\n",
    "    # print(list_of_drugs)\n",
    "    #list_of_drugs = np.unique(list_of_drugs)\n",
    "\n",
    "\n",
    "    # print(list_of_drugs.shape)\n",
    "    return list_of_drugs\n",
    "\n",
    "\n",
    "\n",
    "def get_list_of_cells(path_dataset, dataset):\n",
    "\n",
    "    data = pd.read_csv(path_dataset + dataset + '/cell_protein.csv')\n",
    "\n",
    "    yy = data.iloc[:, 3]\n",
    "    list_of_cells = yy.unique()\n",
    "\n",
    "    data = pd.read_csv(path_dataset + dataset + '/drug_combinations.csv')\n",
    "    if dataset == 'OncologyScreen':\n",
    "      idx = 3\n",
    "    else:\n",
    "      idx = 2\n",
    "    yy = data.iloc[:, idx]\n",
    "    tmp = yy.unique()\n",
    "    list_of_cells = np.concatenate((list_of_cells, tmp))\n",
    "    list_of_cells = np.unique(list_of_cells)\n",
    "    return list_of_cells\n",
    "\n",
    "\n",
    "\n",
    "## Read drug smiles\n",
    "CHARCANSMISET = {\"#\": 1, \"%\": 2, \")\": 3, \"(\": 4, \"+\": 5, \"-\": 6,\n",
    "                 \".\": 7, \"1\": 8, \"0\": 9, \"3\": 10, \"2\": 11, \"5\": 12,\n",
    "                 \"4\": 13, \"7\": 14, \"6\": 15, \"9\": 16, \"8\": 17, \"=\": 18,\n",
    "                 \"A\": 19, \"C\": 20, \"B\": 21, \"E\": 22, \"D\": 23, \"G\": 24,\n",
    "                 \"F\": 25, \"I\": 26, \"H\": 27, \"K\": 28, \"M\": 29, \"L\": 30,\n",
    "                 \"O\": 31, \"N\": 32, \"P\": 33, \"S\": 34, \"R\": 35, \"U\": 36,\n",
    "                 \"T\": 37, \"W\": 38, \"V\": 39, \"Y\": 40, \"[\": 41, \"Z\": 42,\n",
    "                 \"]\": 43, \"_\": 44, \"a\": 45, \"c\": 46, \"b\": 47, \"e\": 48,\n",
    "                 \"d\": 49, \"g\": 50, \"f\": 51, \"i\": 52, \"h\": 53, \"m\": 54,\n",
    "                 \"l\": 55, \"o\": 56, \"n\": 57, \"s\": 58, \"r\": 59, \"u\": 60,\n",
    "                 \"t\": 61, \"y\": 62, \"@\": 63, \"/\": 64, \"\\\\\": 0}\n",
    "\n",
    "\n",
    "def label_smiles(line, MAX_SMI_LEN, smi_ch_ind):\n",
    "    X = np.zeros(MAX_SMI_LEN)\n",
    "    for i, ch in enumerate(line[:MAX_SMI_LEN]):  # x, smi_ch_ind, y\n",
    "        X[i] = smi_ch_ind[ch]\n",
    "\n",
    "    return X  # .tolist()\n",
    "\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "    X = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "    for i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "        X[i] = smi_ch_ind[ch]\n",
    "\n",
    "    return X  # .tolist()\n",
    "\n",
    "\n",
    "smiles_dict_len = 64\n",
    "smiles_max_len = 100\n",
    "\n",
    "\n",
    "def read_protein_protein_graph(path_dataset, dataset, list_of_prots):\n",
    "    data = pd.read_excel(path_dataset + dataset + '/protein-protein_network.xlsx')\n",
    "\n",
    "    protein_protein_graph = nx.Graph()\n",
    "\n",
    "    for i in range(0, data.shape[0]):\n",
    "        tmp = np.argwhere(list_of_prots == data.iloc[i, 0])[0]\n",
    "        tmp1 = np.argwhere(list_of_prots == data.iloc[i, 1])[0]\n",
    "        # data.iloc[i,0:2]\n",
    "        # tmp=np.array(tmp)\n",
    "        protein_protein_graph.add_edge(tmp[0], tmp1[0], weight=1)\n",
    "\n",
    "\n",
    "    return protein_protein_graph\n",
    "\n",
    "\n",
    "def read_protein_protein_matrix(path_dataset, dataset, list_of_prots, list_of_drugs):\n",
    "    data = pd.read_csv(path_dataset + dataset + '/drug_protein.csv')\n",
    "    indx_of_drugs_interaction = []\n",
    "    indx_of_prots_interaction = []\n",
    "    # print(list_of_prots.shape)\n",
    "    protein_drug_matrix = np.zeros((list_of_prots.shape[0], list_of_drugs.shape[0]))\n",
    "    for i in range(0, data.shape[0]):\n",
    "        tmp = np.argwhere(list_of_drugs == data.iloc[i, 0])[0]\n",
    "        # indx_of_drugs_interaction.append(tmp[0])\n",
    "\n",
    "        tmp2 = np.argwhere(list_of_prots == data.iloc[i, 1])[0]\n",
    "        # indx_of_prots_interaction.append(tmp[0])\n",
    "\n",
    "        protein_drug_matrix[tmp2[0], tmp[0]] = 1\n",
    "        # protein_interaction.append(data.iloc[i,1])\n",
    "\n",
    "\n",
    "    return protein_drug_matrix\n",
    "\n",
    "\n",
    "\n",
    "def read_protein_cell_matrix(path_dataset, dataset, list_of_prots, list_of_cells):\n",
    "    data = pd.read_csv(path_dataset + dataset + '/cell_protein.csv')\n",
    "\n",
    "    protein_cell_matrix = np.zeros((list_of_prots.shape[0], list_of_cells.shape[0]))\n",
    "    for i in range(0, data.shape[0]):\n",
    "        tmp = np.argwhere(list_of_prots == data.iloc[i, 2])[0]\n",
    "        tmp2 = np.argwhere(list_of_cells == data.iloc[i, 3])[0]\n",
    "        protein_cell_matrix[tmp[0], tmp2[0]] = 1\n",
    "\n",
    "    return protein_cell_matrix\n",
    "\n",
    "\n",
    "def read_drug_combinations_data(path_dataset, dataset, list_of_drugs, list_of_prots, list_of_cells):\n",
    "    data = pd.read_csv(path_dataset + dataset + '/drug_combinations.csv')\n",
    "\n",
    "    indx_of_drugs1_combinations = []\n",
    "    indx_of_drugs2_combinations = []\n",
    "    indx_of_cells_combinations = []\n",
    "    drugs_combinations = []\n",
    "    num_training_samples = data.shape[0]\n",
    "\n",
    "    # print(sort_lbl.shape[0])\n",
    "    #threshold1 = sort_lbl[17359]\n",
    "    #threshold2 = sort_lbl[52077]\n",
    "    if dataset == 'OncologyScreen':\n",
    "      idx = 1\n",
    "      idx_lbl = 4\n",
    "      idx_cell = 3\n",
    "    else:\n",
    "      idx = 3\n",
    "      idx_lbl = 5\n",
    "      idx_cell = 2\n",
    "    sort_lbl = np.sort(np.array(data.iloc[:, idx_lbl]))\n",
    "    for i in range(0, data.shape[0]):\n",
    "        # print(list_of_drugs)\n",
    "        # print(data.iloc[i, idx])\n",
    "        tmp = np.argwhere(list_of_drugs == data.iloc[i, idx])[0]\n",
    "        indx_of_drugs1_combinations.append(tmp[0])\n",
    "        tmp = np.argwhere(list_of_drugs == data.iloc[i, idx+1])[0]\n",
    "        indx_of_drugs2_combinations.append(tmp[0])\n",
    "        tmp = np.argwhere(list_of_cells == data.iloc[i, idx_cell])[0]\n",
    "        indx_of_cells_combinations.append(tmp[0])\n",
    "        \"\"\"if data.iloc[i,5]<threshold1:\n",
    "           drugs_combinations.append([1,0,0])\n",
    "        if data.iloc[i,5]>threshold2:\n",
    "          drugs_combinations.append([0,0,1])\n",
    "        else:\n",
    "          drugs_combinations.append([0,1,0])\"\"\"\n",
    "        if data.iloc[i, idx_lbl] > np.float64(sort_lbl[int(len(sort_lbl)/2)]):  # (threshold1+threshold2)/2:\n",
    "            drugs_combinations.append([1])\n",
    "        else:\n",
    "            drugs_combinations.append([0])\n",
    "\n",
    "\n",
    "    return drugs_combinations, indx_of_drugs1_combinations, indx_of_drugs2_combinations, indx_of_cells_combinations\n",
    "\n",
    "\n",
    "def apply_clustering_on_proteins(protein_embeddings):\n",
    "\n",
    "    kmeans = KMeans(n_clusters=200, random_state=0).fit(protein_embeddings[0:15970, :])\n",
    "    protein_embeddings = kmeans.cluster_centers_\n",
    "    return protein_embeddings, kmeans\n",
    "\n",
    "\n",
    "def update_protein_drug_matrix_by_clustering(protein_drug_matrix, kmeans):\n",
    "    protein_drug_matrix_kmeans = np.zeros((200, protein_drug_matrix.shape[1]))\n",
    "    for i in range(200):\n",
    "        protein_drug_matrix_kmeans[i, :] = np.max(protein_drug_matrix[kmeans.labels_ == i, :], axis=0)\n",
    "    protein_drug_matrix = protein_drug_matrix_kmeans\n",
    "    return protein_drug_matrix\n",
    "\n",
    "\n",
    "\n",
    "def update_protein_cell_matrix_by_clustering(protein_cell_matrix, kmeans):\n",
    "    protein_cell_matrix_kmeans = np.zeros((200, protein_cell_matrix.shape[1]))\n",
    "    for i in range(200):\n",
    "        protein_cell_matrix_kmeans[i, :] = np.max(protein_cell_matrix[kmeans.labels_ == i, :], axis=0)\n",
    "    protein_cell_matrix = protein_cell_matrix_kmeans\n",
    "    return protein_cell_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JKKpUgQBz0IU",
    "outputId": "ed1502d6-dc44-4681-9bfb-720e6dd79218"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import networkx as nx\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#import tensorflow_addons as tfa\n",
    "from pubchempy import *\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Softmax\n",
    "from keras.layers import Conv2D, GRU\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Masking, RepeatVector, Flatten, \\\n",
    "    Concatenate, Lambda\n",
    "from keras.layers import Dot, GlobalMaxPooling2D, Conv2D, Reshape\n",
    "from keras.models import Model\n",
    "from keras.layers import Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import optimizers, layers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "path_dataset = 'set the path to the folders contain datasets'\n",
    "dataset = 'OncologyScreen'\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#        This part extract the protein protein graph and protein drug graph    #\n",
    "################################################################################\n",
    "\n",
    "list_of_prots = get_list_of_proteins(path_dataset, dataset)\n",
    "\n",
    "list_of_drugs = get_list_of_drugs(path_dataset, dataset)\n",
    "\n",
    "list_of_cells = get_list_of_cells(path_dataset, dataset)\n",
    "\n",
    "protein_protein_graph = read_protein_protein_graph(path_dataset, dataset, list_of_prots)\n",
    "\n",
    "protein_drug_matrix = read_protein_protein_matrix(path_dataset, dataset, list_of_prots, list_of_drugs)\n",
    "\n",
    "protein_cell_matrix = read_protein_cell_matrix(path_dataset, dataset, list_of_prots, list_of_cells)\n",
    "\n",
    "###################################################################################################\n",
    "#     This part of code reads compound ID and get its SMILES sequence and save it as numpy array  #\n",
    "#     For the OncologyScreen dataset and DrugCombDB dataset, smiles sequences are computed and it #\n",
    "#     is available. For your custom dataset, you should comment out these lines to compute the    #\n",
    "#     Smiles based on the their compound Ids.                                                     #\n",
    "###################################################################################################\n",
    "\"\"\"smiles_max_len = 200\n",
    "smiles_unique1 = []\n",
    "for i in range(0, list_of_drugs.shape[0]):\n",
    "    # print(i)\n",
    "    cs = get_compounds(list_of_drugs[i], 'name')\n",
    "    c = Compound.from_cid(cs[0].cid)\n",
    "    #smiles_unique1.append(label_smiles(c.canonical_smiles, smiles_max_len, CHARCANSMISET))\n",
    "    smiles_unique1.append(c.canonical_smiles)\n",
    "# print(list_of_drugs.shape[0])\n",
    "np.save(path_dataset + dataset + '/smiles_sequence.npy',\n",
    "        np.array(smiles_unique1))\"\"\"\n",
    "\n",
    "########################################################################################################\n",
    "#   Read the stored SMILES sequencs\n",
    "smiles_unique = np.load(path_dataset + dataset + '/smiles_200.npy')\n",
    "\n",
    "##########################################################################\n",
    "# Compute similarity between MACCS fingerprint and morgan fingerprints\n",
    "##########################################################################\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "import pandas as pd\n",
    "from molvs import standardize_smiles\n",
    "import numpy as np\n",
    "from rdkit import DataStructs\n",
    "\n",
    "class MACCS:\n",
    "    def __init__(self, smiles):\n",
    "        self.smiles = smiles\n",
    "        self.mols = Chem.MolFromSmiles(self.smiles)\n",
    "        print(self.mols)\n",
    "    def compute_MACCS(self):\n",
    "        MACCS_list = []\n",
    "        #header = ['bit' + str(i) for i in range(167)]\n",
    "        ds = list(MACCSkeys.GenMACCSKeys(self.mols).ToBitString())\n",
    "        MACCS_list.append(ds)\n",
    "        return MACCS_list\n",
    "        #df = pd.DataFrame(MACCS_list,columns=header)\n",
    "        #df.insert(loc=0, column='smiles', value=self.smiles)\n",
    "        #df.to_csv(name[:-4]+'_MACCS.csv', index=False)\n",
    "\n",
    "\n",
    "smiles_sequence = np.load(path_dataset + dataset + '/smiles_sequence.npy')\n",
    "maccs_fp = []\n",
    "morgan_fp = []\n",
    "for i in range(len(smiles_sequence)):\n",
    "    maccs_descriptor = MACCS(standardize_smiles(smiles_sequence[i]))\n",
    "    maccs_fp.append(np.int32(maccs_descriptor.compute_MACCS())[0])\n",
    "    # Load the molecule from the SMILES string\n",
    "    mol = Chem.MolFromSmiles(smiles_sequence[i])\n",
    "\n",
    "    fp1 = AllChem.GetMorganFingerprintAsBitVect(mol, useChirality=True, radius=2, nBits=124)\n",
    "    \n",
    "    vec1 = np.array(fp1)\n",
    "    morgan_fp.append(vec1)\n",
    "\n",
    "def tanimoto_similarity(arr1, arr2):\n",
    "    fp1 = DataStructs.CreateFromBitString(''.join(map(str, arr1)))\n",
    "    fp2 = DataStructs.CreateFromBitString(''.join(map(str, arr2)))\n",
    "    return DataStructs.FingerprintSimilarity(fp1, fp2, metric=DataStructs.TanimotoSimilarity)\n",
    "\n",
    "tanimoto_sim_maccs = np.zeros((len(smiles_sequence), len(smiles_sequence)))\n",
    "tanimoto_sim_morgan = np.zeros((len(smiles_sequence), len(smiles_sequence)))\n",
    "for i in range(0,len(smiles_sequence)):\n",
    "  for j in range(i,len(smiles_sequence)):\n",
    "      tanimoto_sim_maccs[i,j] = tanimoto_similarity(maccs_fp[i], maccs_fp[j])\n",
    "      tanimoto_sim_maccs[j,i] = tanimoto_sim_maccs[i,j]\n",
    "      tanimoto_sim_morgan[i,j] = tanimoto_similarity(morgan_fp[i], morgan_fp[j])\n",
    "      tanimoto_sim_morgan[j,i] = tanimoto_sim_morgan[i,j]\n",
    "# Read drug combinations data\n",
    "drugs_combinations, indx_of_drugs1_combinations, indx_of_drugs2_combinations, indx_of_cells_combinations = read_drug_combinations_data(path_dataset, dataset, list_of_drugs, list_of_prots, list_of_cells)\n",
    "\n",
    "from numpy.linalg import norm\n",
    "cell_similarities_ = np.zeros((len(list_of_cells),len(list_of_cells)))\n",
    "for i in range(0, len(list_of_cells)):\n",
    "  for j in range(i, len(list_of_cells)):\n",
    "    cell_similarities_[i,j] = np.dot(protein_cell_matrix[:,i],protein_cell_matrix[:,j])/(norm(protein_cell_matrix[:,i])*norm(protein_cell_matrix[:,j]))\n",
    "    cell_similarities_[j, i] = cell_similarities_[i,j]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIXSk34SESjF"
   },
   "source": [
    "# New Secti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6sv4DdXcsTv",
    "outputId": "9061c681-68c9-46d0-936c-f3927817f8cb"
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "#          Define Auxiliary functions which are used in model       #\n",
    "####################################################################\n",
    "\n",
    "def dot_batch(inp):\n",
    "\n",
    "    return tf.keras.backend.batch_dot(inp[0], inp[1], axes=(1, 2))  # tf.keras.activations.sigmoid()\n",
    "\n",
    "\n",
    "def dot_batch_axs_pro(inp):\n",
    "\n",
    "    return tf.keras.backend.batch_dot(inp[0], inp[1], axes=(2, 2))\n",
    "\n",
    "\n",
    "def dot_batch_axs_drug(inp):\n",
    "\n",
    "    return tf.keras.activations.sigmoid(tf.keras.backend.batch_dot(inp[0], inp[1], axes=(1, 1)))\n",
    "\n",
    "\n",
    "def dot_batch_axs_cell(inp):\n",
    "\n",
    "    return tf.keras.activations.sigmoid(\n",
    "        tf.keras.backend.batch_dot(inp[0], inp[1], axes=(1, 1)))  # tf.math.multiply(inp[0],inp[1]))\n",
    "\n",
    "\n",
    "def dot_batch_axs_toxic(inp):\n",
    "\n",
    "    return tf.keras.backend.batch_dot(inp[0], inp[1], axes=(1, 1))\n",
    "\n",
    "def compactness_loss(actual, features):\n",
    "    features = Flatten()(features)\n",
    "    k = 2000\n",
    "    batch_size = 10\n",
    "    dim = (batch_size, k)\n",
    "\n",
    "    def zero(i):\n",
    "        z = tf.zeros((1, dim[1]), dtype=tf.dtypes.float32)\n",
    "        o = tf.ones((1, dim[1]), dtype=tf.dtypes.float32)\n",
    "        arr = []\n",
    "        for k in range(dim[0]):\n",
    "            arr.append(o if k != i else z)\n",
    "        res = tf.concat(arr, axis=0)\n",
    "        return res\n",
    "\n",
    "    masks = [zero(i) for i in range(batch_size)]\n",
    "    m = (1 / (batch_size - 1)) * tf.map_fn(\n",
    "        # row-wise summation\n",
    "        lambda mask: tf.math.reduce_sum(features * mask, axis=0),\n",
    "        masks,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    dists = features - m\n",
    "    sqrd_dists = tf.pow(dists, 2)\n",
    "    red_dists = tf.math.reduce_sum(sqrd_dists, axis=1)\n",
    "    compact_loss = (1 / (batch_size * k)) * tf.math.reduce_sum(red_dists)\n",
    "    return compact_loss\n",
    "\n",
    "\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=tf.expand_dims(images, axis=-1),\n",
    "            sizes=[1, self.patch_size / 4, 1, 1],\n",
    "            strides=[1, self.patch_size / 4, 1, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        num_patches = patches.shape[1]\n",
    "        patches1 = patches[:,0:num_patches:2,:,:]\n",
    "        patches2 = patches[:,2:num_patches:2,:,:]\n",
    "        patches3 = patches[:,1:num_patches:2,:,:]\n",
    "        patches4 = patches[:,3:num_patches:2,:,:]\n",
    "\n",
    "        out1 = tf.concat((patches1[:,0:-1,:,:],patches2), axis = -1)\n",
    "        out2 = tf.concat((patches3[:,0:-1,:,:],patches4), axis = -1)\n",
    "        patches_1 = tf.concat((patches[:,0:-2,:,:],patches[:,1:-1,:,:]), axis = -1)\n",
    "\n",
    "        patches_ = tf.concat((out1, out2), axis = 1)\n",
    "        patches = tf.concat((patches_1, patches_), axis = 1)\n",
    "\n",
    "        patch_dims = patches.shape[-1]\n",
    "        # print(patches.shape)\n",
    "\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    # augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#           Feature extractor for Drugs                      #\n",
    "##############################################################\n",
    "# Hyperparameters\n",
    "embedding_size = 20\n",
    "num_filters = 64\n",
    "protein_filter_lengths = 8\n",
    "smiles_filter_lengths = 4\n",
    "smiles_max_len = 200\n",
    "protein_embeding_dim = 200\n",
    "\n",
    "\n",
    "num_classes = 2\n",
    "input_shape = (200, 1)\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "drug_size = 200  # We'll resize input sequences to this size\n",
    "patch_size = 20  # Size of the patches to be extract from the input images\n",
    "num_patches = 38*2#(drug_size // patch_size) * 4\n",
    "projection_dim = 50\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [1024, 1024, 512]  # Size of the dense layers of the final classifier\n",
    "\n",
    "\n",
    "METRICS = [\n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    keras.metrics.AUC(name='prc', curve='PR')  # precision-recall curve\n",
    "]\n",
    "\n",
    "\n",
    "num_prot_cluster = 200\n",
    "\n",
    "# Define Shared Layers\n",
    "Drug1_input = Input(shape=(smiles_max_len, 1), name='drug1_input', dtype='int32')  # dtype='int32',\n",
    "Drug2_input = Input(shape=(smiles_max_len, 1), name='drug2_input', dtype='int32')  # dtype='int32',\n",
    "\n",
    "Drug1_similarities = Input(shape=(len(smiles_sequence),1 ), name='drug1_similarities')\n",
    "Drug2_similarities = Input(shape=(len(smiles_sequence),1 ), name='drug2_similarities')\n",
    "cell_similarities = Input(shape=(len(list_of_cells),), name='cell_similarities')\n",
    "\n",
    "# network for Drugs\n",
    "smiles_embedding = Embedding(input_dim=65, output_dim=128, input_length=200)\n",
    "smiles_first_conv = Conv1D(filters=32, kernel_size=3,  activation='relu', padding='same',  strides=1)\n",
    "smiles_second_conv = Conv1D(filters=32*2, kernel_size=3,  activation='relu', padding='same',  strides=1)\n",
    "smiles_third_conv = Conv1D(filters=32*3, kernel_size=3,  activation='relu', padding='same',  strides=1)\n",
    "smiles_global_pooling = GlobalMaxPooling1D()\n",
    "\n",
    "# Drug1 encoder\n",
    "\n",
    "# network for Drug 1\n",
    "# out1 = Embedding(input_dim=smiles_dict_len+1, output_dim = embedding_size, input_length=smiles_max_len,name='smiles_embedding') (Drug1_input)\n",
    "patches = Patches(patch_size)(Drug1_input)\n",
    "# Encode patches.\n",
    "encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "#Input_encoded_patches = Input(shape = (76,50))\n",
    "# Create multiple layers of the Transformer block.\n",
    "for _ in range(transformer_layers):\n",
    "\n",
    "    num_heads = 1\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "    )(encoded_patches, encoded_patches)\n",
    "    print('--------------------------')\n",
    "    print(encoded_patches.shape)\n",
    "    print(attention_output.shape)\n",
    "    print('***************************')\n",
    "    # Skip connection 1.\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "    # Layer normalization 2.\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "    # MLP.\n",
    "    x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "    # Skip connection 2.\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "    print('--------------------------')\n",
    "    print(encoded_patches.shape)\n",
    "    print(x3.shape)\n",
    "    print(x2.shape)\n",
    "    print('***************************')\n",
    "\n",
    "\n",
    "\n",
    "transformer_model = Model(inputs = [Drug1_input], outputs = [encoded_patches])\n",
    "encode_drug1_ = transformer_model(Drug1_input)\n",
    "encode_drug1 = layers.Flatten()(encode_drug1_[0])\n",
    "\n",
    "num_classes = 2\n",
    "input_shape = (200, 1)\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "drug_size = 200  # We'll resize input sequences to this size\n",
    "patch_size = 20  # Size of the patches to be extract from the input images\n",
    "num_patches = 38*2#(drug_size // patch_size) * 4\n",
    "projection_dim = 50\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [512, 256, 128]\n",
    "\n",
    "patches_2 = Patches(patch_size)(Drug2_input)\n",
    "encoded_patches_2 = PatchEncoder(num_patches, projection_dim)(patches_2)\n",
    "\n",
    "encode_drug2_ = transformer_model([Drug2_input])\n",
    "encode_drug2 = layers.Flatten()(encode_drug2_[0])\n",
    "\n",
    "\"\"\"encode_drug1 = smiles_embedding(Drug1_input)\n",
    "encode_drug1 = smiles_first_conv(encode_drug1)\n",
    "encode_drug1 = smiles_second_conv(encode_drug1)\n",
    "encode_drug1_ = smiles_third_conv(encode_drug1)\n",
    "encode_drug1 = smiles_global_pooling(encode_drug1_)\n",
    "\n",
    "# Drug2 encoder\n",
    "encode_drug2 = smiles_embedding(Drug2_input)\n",
    "encode_drug2 = smiles_first_conv(encode_drug2)\n",
    "encode_drug2 = smiles_second_conv(encode_drug2)\n",
    "encode_drug2_ = smiles_third_conv(encode_drug2)\n",
    "encode_drug2 = smiles_global_pooling(encode_drug2_)\"\"\"\n",
    "\n",
    "sim_mat = Dot(axes=(2,2))([encode_drug1_[0], encode_drug2_[0]])\n",
    "sim_mat = Reshape((76, 76, 1))(sim_mat)\n",
    "sim_mat_first_conv = Conv2D(filters=32, kernel_size=(3, 3),  activation='relu', padding='same',  strides=1)(sim_mat)\n",
    "sim_mat_second_conv = Conv2D(filters=32*2, kernel_size=(3, 3),  activation='relu', padding='same',  strides=1)(sim_mat_first_conv)\n",
    "sim_mat_third_conv = Conv2D(filters=32*3, kernel_size=(3, 3),  activation='relu', padding='same',  strides=1)(sim_mat_second_conv)\n",
    "sim_mat_global_pooling = GlobalMaxPooling2D()(sim_mat_third_conv)\n",
    "\n",
    "\n",
    "# network for drug similarities\n",
    "drugs_sim_dense_1 = Conv1D(filters=32, kernel_size=3,  activation='relu', padding='same',  strides=1)\n",
    "drugs_sim_do_1 = Dropout(0.1)\n",
    "drugs_sim_dense_2 = Conv1D(filters=32*2, kernel_size=3,  activation='relu', padding='same',  strides=1)\n",
    "drugs_sim_do_2 = Conv1D(filters=32*3, kernel_size=3,  activation='relu', padding='same',  strides=1)\n",
    "drugs_sim_dense_3 = GlobalMaxPooling1D()\n",
    "\n",
    "\n",
    "# Drug1\n",
    "drug_sim_1 = drugs_sim_dense_1(Drug1_similarities)\n",
    "drug_sim_1 = drugs_sim_do_1(drug_sim_1)\n",
    "drug_sim_1 = drugs_sim_dense_2(drug_sim_1)\n",
    "drug_sim_1 = drugs_sim_do_2(drug_sim_1)\n",
    "drug_sim_1 = drugs_sim_dense_3(drug_sim_1)\n",
    "\n",
    "# Drug2\n",
    "drugs_sim_dense_1_ = Conv1D(filters=32, kernel_size=3,  activation='relu', padding='same',  strides=1)\n",
    "drugs_sim_dense_2_ = Conv1D(filters=32*2, kernel_size=3,  activation='relu', padding='same',  strides=1)\n",
    "drugs_sim_do_2_ = Conv1D(filters=32*3, kernel_size=3,  activation='relu', padding='same',  strides=1)\n",
    "drugs_sim_dense_3_ = GlobalMaxPooling1D()\n",
    "drugs_sim_do_1_ = Dropout(0.1)\n",
    "\n",
    "\n",
    "drug_sim_2 = drugs_sim_dense_1_(Drug2_similarities)\n",
    "drug_sim_2 = drugs_sim_do_1_(drug_sim_2)\n",
    "drug_sim_2 = drugs_sim_dense_2_(drug_sim_2)\n",
    "drug_sim_2 = drugs_sim_do_2_(drug_sim_2)\n",
    "drug_sim_2 = drugs_sim_dense_3_(drug_sim_2)\n",
    "\n",
    "\n",
    "# network for cell similarities\n",
    "cell_sim_dense_1 = Dense(128)(cell_similarities)\n",
    "cell_sim_do_1 = Dropout(0.2)(cell_sim_dense_1)\n",
    "cell_sim_dense_2 = Dense(64, activation='relu')(cell_sim_do_1)\n",
    "cell_sim_do_2 = Dropout(0.2)(cell_sim_dense_1)\n",
    "cell_sim_dense_3 = Dense(32, activation='relu')(cell_sim_do_2)\n",
    "\n",
    "encode_interaction = keras.layers.concatenate([encode_drug1, encode_drug2, cell_sim_dense_2 , sim_mat_global_pooling, drug_sim_1, drug_sim_2], axis=-1)#, cell_sim_dense_2, sim_mat_global_pooling, drug_sim_1, drug_sim_2\n",
    "# Fully connected\n",
    "FC1 = Dense(1024, kernel_regularizer=keras.regularizers.L1(1e-4))(encode_interaction)\n",
    "FC2 = Dropout(0.1)(FC1)\n",
    "\"\"\"FC2 = Dense(512, activation='relu')(FC2)\n",
    "FC2 = Dropout(0.1)(FC2)\n",
    "FC2 = Dense(256, activation='relu')(FC2)\"\"\"\n",
    "\n",
    "\n",
    "# And add a logistic regression on top\n",
    "predictions = Dense(1,  activation='sigmoid',  name='synergy')(FC2)\n",
    "#Synergy = layers.Dense(1, name='synergy', activation='sigmoid')(features1)\n",
    "# Create the Keras model.\n",
    "model_synergy = keras.Model(inputs=[Drug1_input, Drug2_input,  cell_similarities, Drug1_similarities, Drug2_similarities],#, Drug1_similarities, Drug2_similarities],\n",
    "                            outputs=[predictions])\n",
    "\n",
    "\n",
    "\n",
    "METRICS = [\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    keras.metrics.AUC(name='prc', curve='PR')  # precision-recall curve\n",
    "]\n",
    "adam = tf.optimizers.Adam(learning_rate=0.01)\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=300,\n",
    "    decay_rate=0.9)\n",
    "adam = tf.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# adam=tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model_synergy.compile(optimizer=adam, loss=['binary_crossentropy'], metrics={'synergy': [METRICS]})\n",
    "\n",
    "\n",
    "indx_of_drugs1_combinations = np.array(indx_of_drugs1_combinations)\n",
    "indx_of_drugs2_combinations = np.array(indx_of_drugs2_combinations)\n",
    "drugs_combinations = np.array(drugs_combinations)\n",
    "\n",
    "smiles_unique = np.array(smiles_unique)\n",
    "indx_of_cells_combinations = np.array(indx_of_cells_combinations)\n",
    "\n",
    "choice = np.random.choice(range(indx_of_drugs1_combinations.shape[0]),\n",
    "                          size=(int(indx_of_drugs1_combinations.shape[0] * 0.8),), replace=False)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(indx_of_drugs1_combinations)):\n",
    "    if i == 0:\n",
    "      choice = np.array(list(train_index))\n",
    "\n",
    "ind = np.zeros(indx_of_drugs1_combinations.shape[0], dtype=bool)\n",
    "ind[choice] = True\n",
    "rest = np.argwhere(~ind)\n",
    "\n",
    "# print(choice.shape)\n",
    "# print(rest.shape)\n",
    "\n",
    "############################################\n",
    "#  Splitting data into test and train      #\n",
    "############################################\n",
    "\n",
    "indx_of_drugs1_combinations_train = indx_of_drugs1_combinations[choice]\n",
    "indx_of_drugs2_combinations_train = indx_of_drugs2_combinations[choice]\n",
    "drugs_combinations_train = drugs_combinations[choice, :]\n",
    "indx_of_cells_combinations_train = indx_of_cells_combinations[choice]\n",
    "\n",
    "indx_of_drugs1_combinations_test = indx_of_drugs1_combinations[rest]\n",
    "indx_of_drugs1_combinations_test = indx_of_drugs1_combinations_test.reshape(\n",
    "    (indx_of_drugs1_combinations_test.shape[0],))\n",
    "indx_of_drugs2_combinations_test = indx_of_drugs2_combinations[rest]\n",
    "indx_of_drugs2_combinations_test = indx_of_drugs2_combinations_test.reshape(\n",
    "    (indx_of_drugs2_combinations_test.shape[0],))\n",
    "drugs_combinations_test = drugs_combinations[rest, :]\n",
    "drugs_combinations_test = np.squeeze(drugs_combinations_test)\n",
    "indx_of_cells_combinations_test = indx_of_cells_combinations[rest]\n",
    "indx_of_cells_combinations_test = indx_of_cells_combinations_test.reshape((indx_of_cells_combinations_test.shape[0],))\n",
    "\n",
    "# print(indx_of_cells_combinations_test.shape)\n",
    "# print(drugs_combinations_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(batch_size, indx_of_drugs1_combinations, indx_of_drugs2_combinations,\n",
    "                  indx_of_cells_combinations, drugs_combinations, num_training_samples, flag):\n",
    "    i_c = 0\n",
    "    drugs1 = []\n",
    "    drugs2 = []\n",
    "    combinations = []\n",
    "    cells = []\n",
    "    interactions1 = []\n",
    "    interactions2 = []\n",
    "    idx_perm = np.random.permutation(len(drugs_combinations))\n",
    "    flag = 0\n",
    "    while True:\n",
    "        if i_c >= np.floor(num_training_samples / batch_size):\n",
    "            i_c = 0\n",
    "            flag = 0\n",
    "            idx_perm = np.random.permutation(len(drugs_combinations))\n",
    "        if flag == 0:\n",
    "          drugs1 = smiles_unique[indx_of_drugs1_combinations[idx_perm[i_c * batch_size:(i_c + 1) * batch_size]]]\n",
    "          drugs2 = smiles_unique[indx_of_drugs2_combinations[idx_perm[i_c * batch_size:(i_c + 1) * batch_size]]]\n",
    "          drugs1_sim = tanimoto_sim_maccs[indx_of_drugs1_combinations[idx_perm[i_c * batch_size:(i_c + 1) * batch_size]]]\n",
    "          drugs2_sim = tanimoto_sim_maccs[indx_of_drugs2_combinations[idx_perm[i_c * batch_size:(i_c + 1) * batch_size]]]\n",
    "\n",
    "          combinations = drugs_combinations[idx_perm[i_c * batch_size:(i_c + 1) * batch_size]]\n",
    "          idx = indx_of_cells_combinations[idx_perm[i_c * batch_size:(i_c + 1) * batch_size]]\n",
    "          cell_sim = cell_similarities_[indx_of_cells_combinations[idx_perm[i_c * batch_size:(i_c + 1) * batch_size]]]\n",
    "          \n",
    "\n",
    "          i_c = i_c + 1\n",
    "          flag = 1\n",
    "          yield (drugs1, drugs2, cell_sim, drugs1_sim, drugs2_sim), combinations   #, drugs1_sim, drugs2_sim\n",
    "        else:\n",
    "          flag = 0\n",
    "          yield (drugs2, drugs1, cell_sim, drugs2_sim, drugs1_sim), combinations\n",
    "\n",
    "\n",
    "def generate_data_val(batch_size, indx_of_drugs1_combinations, indx_of_drugs2_combinations,\n",
    "                      indx_of_cells_combinations, drugs_combinations, num_training_samples, flag):\n",
    "    i_c = 0\n",
    "    drugs1 = []\n",
    "    drugs2 = []\n",
    "    combinations = []\n",
    "    cells = []\n",
    "    interactions1 = []\n",
    "    interactions2 = []\n",
    "    while True:\n",
    "        if i_c >= np.floor(num_training_samples / batch_size):\n",
    "            i_c = 0\n",
    "        drugs1 = smiles_unique[indx_of_drugs1_combinations[i_c * batch_size:(i_c + 1) * batch_size]]\n",
    "        drugs2 = smiles_unique[indx_of_drugs2_combinations[i_c * batch_size:(i_c + 1) * batch_size]]\n",
    "        drugs1_sim = np.reshape(tanimoto_sim_maccs[indx_of_drugs1_combinations[i_c * batch_size:(i_c + 1) * batch_size]],\n",
    "                                    (batch_size, -1, 1))\n",
    "        drugs2_sim = np.reshape(tanimoto_sim_maccs[indx_of_drugs2_combinations[i_c * batch_size:(i_c + 1) * batch_size]],\n",
    "                                    (batch_size, -1, 1))\n",
    "        combinations = drugs_combinations[i_c * batch_size:(i_c + 1) * batch_size]\n",
    "        idx = indx_of_cells_combinations[i_c * batch_size:(i_c + 1) * batch_size]\n",
    "        cell_sim = cell_similarities_[indx_of_cells_combinations[i_c * batch_size:(i_c + 1) * batch_size]]\n",
    "        i_c = i_c + 1\n",
    "        yield (np.expand_dims(drugs1, axis=-1), np.expand_dims(drugs2, axis=-1), cell_sim, drugs1_sim, drugs2_sim), combinations   #, drugs1_sim, drugs2_sim\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=300,\n",
    "    decay_rate=0.9)\n",
    "adam = tf.optimizers.Adam(learning_rate=lr_schedule)\n",
    "es = EarlyStopping(monitor=adam, mode='min', verbose=1, patience=15)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='best_model_without_sim_global.keras',\n",
    "    monitor='val_auc',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "model_synergy.fit(\n",
    "\tgenerate_data(100, indx_of_drugs1_combinations_train, indx_of_drugs2_combinations_train,\n",
    "\t\t\t\t  indx_of_cells_combinations_train, drugs_combinations_train,\n",
    "\t\t\t\t  indx_of_drugs1_combinations_train.shape[0], 1),\n",
    "\tsteps_per_epoch=int(np.floor(indx_of_drugs1_combinations_train.shape[0]/100))*2, epochs=100,\n",
    "\tvalidation_data=generate_data_val(10, indx_of_drugs1_combinations_test,\n",
    "\t\t\t\t\t\t\t\t\t  indx_of_drugs2_combinations_test, indx_of_cells_combinations_test,\n",
    "\t\t\t\t\t\t\t\t\t  drugs_combinations_test, indx_of_drugs1_combinations_test.shape[0], 1),\n",
    "\tvalidation_steps=int(np.floor(indx_of_drugs1_combinations_test.shape[0]/10)), callbacks=[model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fV8v-3ytedFL"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred_prob = model_synergy.predict(generate_data_val(10, indx_of_drugs1_combinations_test,\n",
    "\t\t\t\t\t\t\t\t\t  indx_of_drugs2_combinations_test, indx_of_cells_combinations_test,\n",
    "\t\t\t\t\t\t\t\t\t  drugs_combinations_test, indx_of_drugs1_combinations_test.shape[0], 1), steps = int(np.floor(indx_of_drugs1_combinations_test.shape[0]/10)))\n",
    "\n",
    "# Calculate ROC curve\n",
    "y_test = drugs_combinations_test\n",
    "fpr, tpr, _ = roc_curve(y_test[0:len(y_pred_prob)], y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
